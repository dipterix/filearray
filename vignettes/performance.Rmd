---
title: "Performance Comparisons - (Numerical)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Performance Comparisons - (Numerical)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = FALSE,
  comment = "#>"
)
```

This vignette profiles `FileArray` operations and compares with R native functions. The goal is to

1. Benchmark different ways to operate on file arrays (write, read, subset, transform, ...)
2. Compare with other methods to see what file array can and cannot do.

The simulation was performed on `MacBook Air 2020 (M1 Chip, ARM, 8GB RAM)`, with R `4.1.0`. To reproduce the results, please install `CRAN` packages `dipsaus` and `microbenchmark`.


## Setup

We mainly test the performance of `double` and `float` data type. The dimensions for both arrays are `100x100x100x100`. Both arrays are around `800MB` in native R. This is because R does not have float precision. However, while `double` array occupies `800MB` space on the hard disk, `float` array only uses half size (`400MB`).

```{r setup}
library(filearray)

filearray_threads()
#> [1] 8

# Create file array and initialize partitions
set.seed(1)
file <- tempfile(); unlink(file, recursive = TRUE)
x_dbl <- filearray_create(file, rep(100, 4))
x_dbl$initialize_partition()

file <- tempfile(); unlink(file, recursive = TRUE)
x_flt <- filearray_create(file, rep(100, 4), type = 'float')
x_flt$initialize_partition()

# 800 MB double array
y <- array(rnorm(length(x_dbl)), dim(x_dbl))
```

## Simulation

The simulation contains

* Write speed
  - Writing along margin
  - Writing chunks of data
* Real speed
  - Read all the data
  - Read along margins
  - Random subset
* Collapse

### 1. Write along margin

Writing along margins refer to something like `x[,,,i] <- v` (along the last margin), or `x[,i,,] <- v` (along the second margin). It is always recommended to write along the last margin, and always discouraged to write along the first margin to file arrays.

1.  partition margin

```{r}
microbenchmark::microbenchmark(
  double = {
    for(i in 1:100){
      x_dbl[,,,i] <- y[,,,i]
    }
  },
  float = {
    for(i in 1:100){
      x_flt[,,,i] <- y[,,,i]
    }
  }, unit = 's', times = 3
)

#> Unit: seconds
#>    expr       min        lq     mean    median       uq      max neval
#>  double 1.4115227 1.5130041 1.695382 1.6144855 1.837312 2.060138     3
#>   float 0.9400064 0.9661546 1.056188 0.9923029 1.114279 1.236255     3
```

2. Write along fast margin

```{r}
microbenchmark::microbenchmark(
  double = {
    for(i in 1:100){
      x_dbl[,,i,] <- y[,,i,]
    }
  },
  float = {
    for(i in 1:100){
      x_flt[,,i,] <- y[,,i,]
    }
  }, unit = 's', times = 3
)

#> Unit: seconds
#>    expr      min       lq     mean   median       uq      max neval
#>  double 1.553554 1.618782 3.233683 1.684009 4.073747 6.463485     3
#>   float 1.297981 1.302451 1.385429 1.306921 1.429154 1.551386     3
```

3. Writing along slow margin

Since it'll take long to run, I only wrote first 10 slices, and multiply by 10 to approximate.

```{r}
system.time({
  for(i in 1:10){
    x_dbl[1,,,] <- y[1,,,]
  }
}) * 10
#>    user  system elapsed 
#>   20.09  312.84   53.89
```


### 2. Write chunks of data

Instead of writing one slice at a time along each margin, we write `100x100x100x5` (10 slices) each time.

1.  partition margin

```{r}
microbenchmark::microbenchmark(
  double = {
    for(i in 1:10){
      idx <- (i-1)*10 + 1:10
      x_dbl[,,,idx] <- y[,,,idx]
    }
  },
  float = {
    for(i in 1:10){
      idx <- (i-1)*10 + 1:10
      x_flt[,,,idx] <- y[,,,idx]
    }
  }, unit = 's', times = 3
)

#> Unit: seconds
#>    expr       min        lq      mean    median       uq      max neval
#>  double 1.1543086 1.1656411 1.1777855 1.1769736 1.189524 1.202074     3
#>   float 0.8987737 0.9232518 0.9699052 0.9477298 1.005471 1.063212     3
```

2. Write along fast margin

```{r}
microbenchmark::microbenchmark(
  double = {
    for(i in 1:10){
      idx <- (i-1)*10 + 1:10
      x_dbl[,,idx,] <- y[,,idx,]
    }
  },
  float = {
    for(i in 1:10){
      idx <- (i-1)*10 + 1:10
      x_flt[,,idx,] <- y[,,idx,]
    }
  }, unit = 's', times = 3
)

#> Unit: seconds
#>    expr      min       lq     mean   median       uq      max neval
#>  double 1.866938 1.889742 1.903477 1.912545 1.921747 1.930949     3
#>   float 1.148298 1.218198 1.258743 1.288098 1.313966 1.339833     3
```

3. Writing along slow margin

```{r}
microbenchmark::microbenchmark(
  double = {
    for(i in 1:10){
      idx <- (i-1)*10 + 1:10
      x_dbl[idx,,,] <- y[idx,,,]
    }
  },
  float = {
    for(i in 1:10){
      idx <- (i-1)*10 + 1:10
      x_flt[idx,,,] <- y[idx,,,]
    }
  }, unit = 's', times = 3
)
#> Unit: seconds
#>    expr      min       lq     mean  median       uq      max neval
#>  double 6.216564 6.305727 6.495289 6.39489 6.634652 6.874414     3
#>   float 3.755463 3.801041 3.856988 3.84662 3.907751 3.968882     3
```

Note: always avoid writing along the first margin (like `x[i,,,] <- ...`). It's slow.



## Read 

### 1. Read the whole array

```{r}
microbenchmark::microbenchmark(
  double = { x_dbl[] },
  float = { x_flt[] },
  unit = 's', times = 3
)

#> Unit: seconds
#>    expr       min        lq      mean    median        uq       max neval
#>  double 0.1255119 0.1825065 0.2082074 0.2395010 0.2495552 0.2596094     3
#>   float 0.1182873 0.1566398 0.1827462 0.1949923 0.2149756 0.2349589     3
```

### 2. Read along margins

```{r}
microbenchmark::microbenchmark(
  farr_double_partition_margin = { x_dbl[,,,1] },
  farr_double_fast_margin = { x_dbl[,,1,] },
  farr_double_slow_margin = { x_dbl[1,,,] },
  farr_float_partition_margin = { x_flt[,,,1] },
  farr_float_fast_margin = { x_flt[,,1,] },
  farr_float_slow_margin = { x_flt[1,,,] },
  native_partition_margin = { y[,,,1] },
  native_fast_margin = { y[,,1,] },
  native_slow_margin = { y[1,,,] },
  times = 100L, unit = "ms"
)

#> Unit: milliseconds
#>                          expr       min        lq      mean    median        uq        max neval
#>  farr_double_partition_margin  1.792438  2.308669  5.369872  2.711576  3.744694 195.816943   100
#>       farr_double_fast_margin  1.404045  1.699532  3.620348  2.021320  3.628787  93.920340   100
#>       farr_double_slow_margin 22.748686 23.529387 37.885944 24.545593 26.726116 299.898108   100
#>   farr_float_partition_margin  1.226761  2.018533  3.364908  2.463833  3.448735  22.872424   100
#>        farr_float_fast_margin  1.267064  1.627638  5.287114  1.990550  3.370815 218.804741   100
#>        farr_float_slow_margin 13.349108 14.117899 26.336786 15.020063 17.870629 220.553145   100
#>       native_partition_margin  3.535594  4.034790  4.469297  4.152910  4.642656   8.365107   100
#>            native_fast_margin  3.519645  4.035281  4.431636  4.111726  4.660654   9.594041   100
#>            native_slow_margin 21.322911 22.716686 28.260453 23.391484 25.876576 102.389341   100
```

The file array indexing is tied or even faster than native arrays in R!

### 3. Random access

```{r}
# access 50 x 50 x 50 x 50 sub-array, with random indices
idx1 <- sample(1:100, 50)
idx2 <- sample(1:100, 50)
idx3 <- sample(1:100, 50)
idx4 <- sample(1:100, 50)

microbenchmark::microbenchmark(
  farr_double = { x_dbl[idx1, idx2, idx3, idx4] },
  farr_float = { x_flt[idx1, idx2, idx3, idx4] },
  native = { y[idx1, idx2, idx3, idx4] },
  times = 100L, unit = "ms"
)

#> Unit: milliseconds
#>         expr       min       lq     mean   median       uq      max neval
#>  farr_double 14.009085 15.27236 23.24250 15.69168 17.29390 136.5074   100
#>   farr_float  8.916229 10.09071 15.33173 10.67378 11.47514 162.7953   100
#>       native 31.193948 32.37274 36.27889 32.89500 33.75403 105.6884   100
```

Random access could be faster than base R (also much less memory!)

## Collapse

Collapse calculates the margin sum/mean. Collapse function in `filearray` uses single thread. This is because the bottle-neck often comes from hard-disk accessing speed. However, it is still faster than native R, and is more memory-efficient.

```{r}
keep <- c(2, 4)
microbenchmark::microbenchmark(
  farr_double = { x_dbl$collapse(keep = keep, method = "sum") },
  farr_float = { x_flt$collapse(keep = keep, method = "sum") },
  native = { apply(y, keep, sum) },
  dipsaus = { dipsaus::collapse(y, keep, average = FALSE) }, 
  unit = "s", times = 5
)

#> Unit: milliseconds
#>                          expr       min        lq      mean    median        uq        max neval
#>  farr_double_partition_margin  1.792438  2.308669  5.369872  2.711576  3.744694 195.816943   100
#>       farr_double_fast_margin  1.404045  1.699532  3.620348  2.021320  3.628787  93.920340   100
#>       farr_double_slow_margin 22.748686 23.529387 37.885944 24.545593 26.726116 299.898108   100
#>   farr_float_partition_margin  1.226761  2.018533  3.364908  2.463833  3.448735  22.872424   100
#>        farr_float_fast_margin  1.267064  1.627638  5.287114  1.990550  3.370815 218.804741   100
#>        farr_float_slow_margin 13.349108 14.117899 26.336786 15.020063 17.870629 220.553145   100
#>       native_partition_margin  3.535594  4.034790  4.469297  4.152910  4.642656   8.365107   100
#>            native_fast_margin  3.519645  4.035281  4.431636  4.111726  4.660654   9.594041   100
#>            native_slow_margin 21.322911 22.716686 28.260453 23.391484 25.876576 102.389341   100
```

The `dipsaus` package uses multiple threads to collapse arrays in-memory. It is `6x` as fast as base R. File array is `2x` as fast as base R.
